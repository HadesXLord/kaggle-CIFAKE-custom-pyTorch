{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "254dc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3756b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## self attention\n",
    "def attention(embedded_matrix):\n",
    "    d_model=embedded_matrix.shape[1]\n",
    "    dk=d_model\n",
    "\n",
    "    wq=np.random.rand(d_model,dk)\n",
    "    wk=np.random.rand(d_model,dk)\n",
    "    wv=np.random.rand(d_model,dk)\n",
    "\n",
    "    Q=embedded_matrix @ wq\n",
    "    K=embedded_matrix @ wk\n",
    "    V=embedded_matrix @ wv\n",
    "\n",
    "    s_scores=tf.nn.softmax(Q @ K.T/np.sqrt(dk),axis=-1)\n",
    "    attention=s_scores @ V\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "518dce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multihead self attention\n",
    "def multihead_attention(embedded_matrix,head): ##embedded matrix is the list of encoded words in the sentence \n",
    "    em_dimension=embedded_matrix.shape[1]\n",
    "    dk=em_dimension//head\n",
    "    heads_output=[]\n",
    "    for i in range(head):\n",
    "        wq=np.random.rand(em_dimension,dk)\n",
    "        wk=np.random.rand(em_dimension,dk)\n",
    "        wv=np.random.rand(em_dimension,dk)\n",
    "\n",
    "        # Q=np.dot(embedded_matrix,wq)\n",
    "        # K=np.dot(embedded_matrix,wk)\n",
    "        # V=np.dot(embedded_matrix,wv)\n",
    "\n",
    "        Q,K,V=embedded_matrix @ wq,embedded_matrix @ wk,embedded_matrix @ wv\n",
    "        \n",
    "        # S=np.dot(Q,K.T)\n",
    "        # s_scores=tf.nn.softmax(tf.constant(S/np.sqrt(dk)),axis=-1)\n",
    "        s_scores=tf.nn.softmax(Q @ K.T/np.sqrt(dk),axis=-1)\n",
    "        head_output=np.dot(s_scores,V)\n",
    "        heads_output.append(head_output)\n",
    "\n",
    "    multihead_output=np.concatenate(heads_output,axis=-1)\n",
    "    return multihead_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8a734e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimised multihead self attention using tensorflow\n",
    "\n",
    "def tf_optimised_multihead_attention(embedded_matrix,head):\n",
    "    seq_len,d_model=embedded_matrix.shape\n",
    "    dk=d_model//head\n",
    "\n",
    "    wq=tf.random.normal((d_model,d_model))\n",
    "    wk=tf.random.normal((d_model,d_model))\n",
    "    wv=tf.random.normal((d_model,d_model))\n",
    "\n",
    "    Q=tf.matmul(embedded_matrix,wq)\n",
    "    K=tf.matmul(embedded_matrix,wk)\n",
    "    V=tf.matmul(embedded_matrix,wv)\n",
    "\n",
    "    Q=tf.reshape(Q,(seq_len,head,dk))\n",
    "    K=tf.reshape(K,(seq_len,head,dk))\n",
    "    V=tf.reshape(V,(seq_len,head,dk))\n",
    "\n",
    "    Q=tf.transpose(Q,(1,0,2))\n",
    "    K=tf.transpose(K,(1,0,2))\n",
    "    V=tf.transpose(V,(1,0,2))\n",
    "\n",
    "    print(Q.shape)\n",
    "\n",
    "    attention=tf.matmul(tf.nn.softmax(tf.matmul(Q,K,transpose_b=True)/tf.sqrt(float(dk)),axis=-1),V)\n",
    "    attention=tf.transpose(attention,(1,0,2))\n",
    "    attention=tf.reshape(attention,(seq_len,d_model))\n",
    "    return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f15c9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multihead self attention using pytorch\n",
    "\n",
    "def torch_optimised_multihead_attention(embedded_matrix,head):\n",
    "    seq_len,d_model=embedded_matrix.shape\n",
    "    dk=d_model//head\n",
    "\n",
    "    torch.manual_seed(0) ## ?\n",
    "    wq=torch.randn(d_model,d_model)\n",
    "    wk=torch.randn(d_model,d_model)\n",
    "    wv=torch.randn(d_model,d_model)\n",
    "\n",
    "    Q=torch.matmul(embedded_matrix,wq).reshape(seq_len,head,dk).transpose(0,1)\n",
    "    K=torch.matmul(embedded_matrix,wk).reshape(seq_len,head,dk).transpose(0,1)\n",
    "    V=torch.matmul(embedded_matrix,wv).reshape(seq_len,head,dk).transpose(0,1)\n",
    "\n",
    "    # Q=Q.reshape(seq_len,head,dk).transpose(0,1)\n",
    "    # K=K.reshape(seq_len,head,dk).transpose(0,1)\n",
    "    # V=V.reshape(seq_len,head,dk).transpose(0,1)\n",
    "\n",
    "    attention=torch.matmul(torch.nn.functional.softmax(torch.matmul(Q,K.transpose(1,2))/dk**0.5),V)\n",
    "    attention=attention.transpose(0,1).reshape(seq_len,d_model)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5ad2640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h4/hyzcmkfj6znf3h1xgg6qs93r0000gn/T/ipykernel_7604/3816852204.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention=torch.matmul(torch.nn.functional.softmax(torch.matmul(Q,K.transpose(1,2))/dk**0.5),V)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.286448</td>\n",
       "      <td>-7.218716</td>\n",
       "      <td>-65.536964</td>\n",
       "      <td>-0.393804</td>\n",
       "      <td>-22.461563</td>\n",
       "      <td>6.108440</td>\n",
       "      <td>26.791262</td>\n",
       "      <td>2.077224</td>\n",
       "      <td>-34.954956</td>\n",
       "      <td>-21.879604</td>\n",
       "      <td>...</td>\n",
       "      <td>-61.047943</td>\n",
       "      <td>-39.060551</td>\n",
       "      <td>25.494331</td>\n",
       "      <td>14.293823</td>\n",
       "      <td>23.726423</td>\n",
       "      <td>27.522730</td>\n",
       "      <td>-12.154991</td>\n",
       "      <td>-17.126074</td>\n",
       "      <td>-13.940536</td>\n",
       "      <td>-27.326046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.212711</td>\n",
       "      <td>46.038944</td>\n",
       "      <td>26.101774</td>\n",
       "      <td>13.921482</td>\n",
       "      <td>-3.878663</td>\n",
       "      <td>-32.376938</td>\n",
       "      <td>26.782457</td>\n",
       "      <td>-0.176930</td>\n",
       "      <td>-13.745987</td>\n",
       "      <td>-29.037703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.871656</td>\n",
       "      <td>-0.509242</td>\n",
       "      <td>-32.699669</td>\n",
       "      <td>23.849894</td>\n",
       "      <td>14.592422</td>\n",
       "      <td>-24.102676</td>\n",
       "      <td>-5.402025</td>\n",
       "      <td>13.767224</td>\n",
       "      <td>-13.874695</td>\n",
       "      <td>-7.470405</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.290718</td>\n",
       "      <td>28.004072</td>\n",
       "      <td>21.878265</td>\n",
       "      <td>30.222874</td>\n",
       "      <td>50.605042</td>\n",
       "      <td>-51.244186</td>\n",
       "      <td>1.379009</td>\n",
       "      <td>15.707146</td>\n",
       "      <td>-43.345650</td>\n",
       "      <td>-20.717903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.411276</td>\n",
       "      <td>-10.957131</td>\n",
       "      <td>-15.481497</td>\n",
       "      <td>-8.431929</td>\n",
       "      <td>15.860730</td>\n",
       "      <td>-1.524310</td>\n",
       "      <td>32.276199</td>\n",
       "      <td>-14.779412</td>\n",
       "      <td>-8.207357</td>\n",
       "      <td>-25.022463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.009623</td>\n",
       "      <td>-0.018298</td>\n",
       "      <td>-0.017522</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>-0.003066</td>\n",
       "      <td>0.022919</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>-0.022592</td>\n",
       "      <td>0.006021</td>\n",
       "      <td>-0.009234</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.560917</td>\n",
       "      <td>-3.401998</td>\n",
       "      <td>2.626210</td>\n",
       "      <td>5.854098</td>\n",
       "      <td>-0.343407</td>\n",
       "      <td>-0.398744</td>\n",
       "      <td>2.144411</td>\n",
       "      <td>-2.582226</td>\n",
       "      <td>-3.192560</td>\n",
       "      <td>-0.843531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0   2.286448  -7.218716 -65.536964  -0.393804 -22.461563   6.108440   \n",
       "1   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2  16.871656  -0.509242 -32.699669  23.849894  14.592422 -24.102676   \n",
       "3  31.411276 -10.957131 -15.481497  -8.431929  15.860730  -1.524310   \n",
       "4  -0.009623  -0.018298  -0.017522   0.003621  -0.003066   0.022919   \n",
       "\n",
       "         6          7          8          9    ...        502        503  \\\n",
       "0  26.791262   2.077224 -34.954956 -21.879604  ... -61.047943 -39.060551   \n",
       "1   0.000000   0.000000   0.000000   0.000000  ... -21.212711  46.038944   \n",
       "2  -5.402025  13.767224 -13.874695  -7.470405  ... -21.290718  28.004072   \n",
       "3  32.276199 -14.779412  -8.207357 -25.022463  ...   0.000000   0.000000   \n",
       "4   0.008482  -0.022592   0.006021  -0.009234  ... -14.560917  -3.401998   \n",
       "\n",
       "         504        505        506        507        508        509  \\\n",
       "0  25.494331  14.293823  23.726423  27.522730 -12.154991 -17.126074   \n",
       "1  26.101774  13.921482  -3.878663 -32.376938  26.782457  -0.176930   \n",
       "2  21.878265  30.222874  50.605042 -51.244186   1.379009  15.707146   \n",
       "3   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4   2.626210   5.854098  -0.343407  -0.398744   2.144411  -2.582226   \n",
       "\n",
       "         510        511  \n",
       "0 -13.940536 -27.326046  \n",
       "1 -13.745987 -29.037703  \n",
       "2 -43.345650 -20.717903  \n",
       "3   0.000000   0.000000  \n",
       "4  -3.192560  -0.843531  \n",
       "\n",
       "[5 rows x 512 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_matrix_tf=tf.random.normal((5,512)) # 5 word of 512 dimensions (original transfomers use 512 dimensions embedding)\n",
    "embedded_matrix_torch=torch.randn((5,512))\n",
    "# embedded_matrix_torch=torch.tensor(embedded_matrix_tf)\n",
    "head=8                                    #head cout=8 (original transformers uses 8 headed self attention)\n",
    "encoded_y_torch=pd.DataFrame(torch_optimised_multihead_attention(embedded_matrix_torch,head))\n",
    "encoded_y_tf=pd.DataFrame(tf_optimised_multihead_attention(embedded_matrix_tf,head))\n",
    "\n",
    "encoded_y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7c77f209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.864044</td>\n",
       "      <td>37.820801</td>\n",
       "      <td>33.560844</td>\n",
       "      <td>-18.382177</td>\n",
       "      <td>13.682326</td>\n",
       "      <td>14.159550</td>\n",
       "      <td>3.782744</td>\n",
       "      <td>37.861504</td>\n",
       "      <td>28.479715</td>\n",
       "      <td>30.034122</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.948334</td>\n",
       "      <td>1.257328</td>\n",
       "      <td>14.466679</td>\n",
       "      <td>13.083627</td>\n",
       "      <td>-26.345745</td>\n",
       "      <td>39.289833</td>\n",
       "      <td>21.016628</td>\n",
       "      <td>18.336861</td>\n",
       "      <td>45.738056</td>\n",
       "      <td>25.368822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.741022</td>\n",
       "      <td>-30.424591</td>\n",
       "      <td>28.032730</td>\n",
       "      <td>-14.077796</td>\n",
       "      <td>26.095919</td>\n",
       "      <td>-37.326965</td>\n",
       "      <td>-16.796021</td>\n",
       "      <td>-1.663698</td>\n",
       "      <td>-17.564228</td>\n",
       "      <td>16.308138</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.720152</td>\n",
       "      <td>25.720636</td>\n",
       "      <td>35.871601</td>\n",
       "      <td>20.399868</td>\n",
       "      <td>20.668680</td>\n",
       "      <td>18.890152</td>\n",
       "      <td>-23.575016</td>\n",
       "      <td>10.963066</td>\n",
       "      <td>-4.893365</td>\n",
       "      <td>13.514553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.684231</td>\n",
       "      <td>41.466179</td>\n",
       "      <td>-12.864634</td>\n",
       "      <td>10.458164</td>\n",
       "      <td>23.111021</td>\n",
       "      <td>0.891163</td>\n",
       "      <td>-9.494844</td>\n",
       "      <td>-15.491872</td>\n",
       "      <td>-23.477005</td>\n",
       "      <td>-23.491817</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.792551</td>\n",
       "      <td>-13.303019</td>\n",
       "      <td>6.409123</td>\n",
       "      <td>77.613075</td>\n",
       "      <td>18.483149</td>\n",
       "      <td>-25.753014</td>\n",
       "      <td>24.391832</td>\n",
       "      <td>6.413098</td>\n",
       "      <td>-22.439007</td>\n",
       "      <td>-13.136946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.864044</td>\n",
       "      <td>37.820801</td>\n",
       "      <td>33.560844</td>\n",
       "      <td>-18.382177</td>\n",
       "      <td>13.682326</td>\n",
       "      <td>14.159550</td>\n",
       "      <td>3.782744</td>\n",
       "      <td>37.861504</td>\n",
       "      <td>28.479715</td>\n",
       "      <td>30.034122</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.948334</td>\n",
       "      <td>1.257328</td>\n",
       "      <td>14.466679</td>\n",
       "      <td>13.083627</td>\n",
       "      <td>-26.345745</td>\n",
       "      <td>39.289833</td>\n",
       "      <td>21.016628</td>\n",
       "      <td>18.336861</td>\n",
       "      <td>45.738056</td>\n",
       "      <td>25.368822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-29.682817</td>\n",
       "      <td>-9.594486</td>\n",
       "      <td>-23.035667</td>\n",
       "      <td>-6.676290</td>\n",
       "      <td>17.824190</td>\n",
       "      <td>-16.444164</td>\n",
       "      <td>4.947160</td>\n",
       "      <td>-20.572647</td>\n",
       "      <td>27.474737</td>\n",
       "      <td>-13.265723</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.634958</td>\n",
       "      <td>42.192703</td>\n",
       "      <td>-8.291959</td>\n",
       "      <td>-20.961369</td>\n",
       "      <td>-2.794889</td>\n",
       "      <td>16.876553</td>\n",
       "      <td>21.521496</td>\n",
       "      <td>-3.419733</td>\n",
       "      <td>-20.114613</td>\n",
       "      <td>14.617207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0   5.864044  37.820801  33.560844 -18.382177  13.682326  14.159550   \n",
       "1  -2.741022 -30.424591  28.032730 -14.077796  26.095919 -37.326965   \n",
       "2  35.684231  41.466179 -12.864634  10.458164  23.111021   0.891163   \n",
       "3   5.864044  37.820801  33.560844 -18.382177  13.682326  14.159550   \n",
       "4 -29.682817  -9.594486 -23.035667  -6.676290  17.824190 -16.444164   \n",
       "\n",
       "         6          7          8          9    ...        502        503  \\\n",
       "0   3.782744  37.861504  28.479715  30.034122  ... -30.948334   1.257328   \n",
       "1 -16.796021  -1.663698 -17.564228  16.308138  ... -12.720152  25.720636   \n",
       "2  -9.494844 -15.491872 -23.477005 -23.491817  ...  -2.792551 -13.303019   \n",
       "3   3.782744  37.861504  28.479715  30.034122  ... -30.948334   1.257328   \n",
       "4   4.947160 -20.572647  27.474737 -13.265723  ... -31.634958  42.192703   \n",
       "\n",
       "         504        505        506        507        508        509  \\\n",
       "0  14.466679  13.083627 -26.345745  39.289833  21.016628  18.336861   \n",
       "1  35.871601  20.399868  20.668680  18.890152 -23.575016  10.963066   \n",
       "2   6.409123  77.613075  18.483149 -25.753014  24.391832   6.413098   \n",
       "3  14.466679  13.083627 -26.345745  39.289833  21.016628  18.336861   \n",
       "4  -8.291959 -20.961369  -2.794889  16.876553  21.521496  -3.419733   \n",
       "\n",
       "         510        511  \n",
       "0  45.738056  25.368822  \n",
       "1  -4.893365  13.514553  \n",
       "2 -22.439007 -13.136946  \n",
       "3  45.738056  25.368822  \n",
       "4 -20.114613  14.617207  \n",
       "\n",
       "[5 rows x 512 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_y_tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
